\chapter{Digital Filters}
\label{ch:digital_filters}

Conceived in general terms, a filter is a physical device for removing unwanted components of a mixture. In the technical field a filter is a system designed to extract information from noisy, distorted data. That is, the filter delivers an estimate of the variables of principal interest, which is why it may also be called an estimator. Filter theory is applied in diverse fields of science and technology, such as communications, radar, sonar, navigation, and biomedical engineering \cite{haykin2002adaptive}.

In contrast to analogue filters that consist of electronic circuits to attenuate unwanted frequencies in continuous-time signals and thus extracted the useful signal, a digital filter is a set of mathematical operations applied to a discrete-time signal in order to extract information about the hidden quantity of interest. A discrete-time signal is a sequence of samples at equidistant time instants that represent the continuous-time signal with no loss, provided the sampling theorem is satisfied, according to which the sample frequency has to be greater than twice the highest frequency component of the continuous-time signal.

Digital filters can be classified as linear and nonlinear. If the quantity at the output of the filter is a linear function of its input, that is, the filter function satisfies the superposition principle, the filter is said to be linear. Otherwise, the filter is nonlinear.

\section{The Filtering Problem}

Consider, as an example involving filter theory, the continuous-time dynamical system depicted in Figure \ref{fig:state_estimation}. The desired state vector of the system, \gls{not:x(t)_v}, is usually hidden and can only be observed by indirect measurements \gls{not:y(t)_v} that are a function of \gls{not:x(t)_v} and subject to noise. Equally, the equation describing the evolution of the state \gls{not:x(t)_v} is usually subject to system errors. The dynamical system may be an aircraft in flight, in which case the elements of the state vector are constituted by its position and velocity. The measuring system may be a tracking radar producing the observation vector \gls{not:y(t)_v} over an interval $[0, T]$. The requirement is to deliver a reliable estimate \gls{not:x_hat(t)_v} of the actual state \gls{not:x(t)_v}, taking prior information into account.

\tikzstyle{block} = [draw, rectangle, minimum height=3em, minimum width=6em]
\tikzstyle{output} = [coordinate]
\tikzstyle{pinstyle} = [pin edge={to-, thin, black}, align=center]

\begin{figure}
\centering
\begin{tikzpicture}[auto, node distance=3cm,>=latex']
    \node [block, align=center, 
    	pin={[pinstyle]below:System \\ Errors}]
    	(dynamical) {Dynamical \\ system};
    \node [block, align=center, right of=dynamical, pin={[pinstyle]below:Measurement \\ errors}, node distance=4.5cm] (measuring) {Measuring \\ system};
    \node [block, align=center, right of=measuring, pin={[pinstyle]below:Prior \\ information}, node distance=4.5cm] (estimator) {Estimator};
    \node [output, right of=estimator] (output) {};
    
    \draw [->, align=center] (dynamical) -- node[name=x] {State \\ \gls{not:x(t)_v}} (measuring);
    \draw [->, align=center] (measuring) -- node[name=y] {Observation \\ \gls{not:y(t)_v}} (estimator);
    \draw [->, align=center] (estimator) -- node[name=y] {Estimate \\ of state \\ $\hat{\mathbf{x}}(t)$} (output);
\end{tikzpicture}
\caption{Block diagram depicting the components involved in state estimation, adopted from \cite{haykin2002adaptive}.} \label{fig:state_estimation}
\end{figure}

\section{Wiener Filter}

A statistical criterion, according to which the performance of a filter can be measured, is the mean-squared error. Consider the linear dicrete-time filter with the impulse response $w_0, w_1, w_2, \dots$ depicted in Figure \ref{fig:filtering_problem}. At some discrete time $n$ it produces an output designated by $y(n)$, which provides an estimate of a desired response denoted by $d(n)$. According to \citeauthor{haykin2002adaptive} \cite{haykin2002adaptive} the essence of the filtering problem and the resulting requirement is summarised with the following statement:

\begin{quote}``Design a linear discrete-time filter whose output $y(n)$ provides an estimate of the desired response $d(n)$, given a set of input samples $u(0), u(1), u(2), \dots$, such that the mean-square value of the estimation error $e(n)$, defined as the difference between the desired response $d(n)$ and the actual response $y(n)$, is minimized.''
\end{quote}

\tikzstyle{block} = [draw, rectangle, 
    minimum height=2.6cm, minimum width=3.2cm]
\tikzstyle{sum} = [draw, circle, node distance=4cm]
\tikzstyle{input} = [coordinate]
\tikzstyle{output} = [coordinate]
\tikzstyle{error} = [coordinate]
\tikzstyle{pinstyle} = [pin edge={to-,thin,black}]

\begin{figure}
\centering
\begin{tikzpicture}[auto, node distance=3cm,>=latex']
    
\node [input, name=input] {};
\node [block, align=left, right of=input, node distance=4.4cm] (filter) {Linear \\ discrete-time \\ filter \\ $w_0, w_1, w_2, \dots$};
\node [sum, right of=filter, node distance=4.4cm] (sum) {$\sum$};	
\node [output, right of=sum, node distance=2.8cm, name=output] {};
\node [error, below of=sum, node distance=2.8cm, name=error] {};

\draw [draw,->, align=left] (input) -- node [pos=0.29]{Input \\ $u(0), u(1), u(2), \dots$} (filter);
\draw [draw,->, align=left] (filter) -- node {Output \\ $y(n)$} node[pos=0.9, label={below:$-$}] {} (sum);
\draw [draw,->, align=left] (output) -- node [label={above:Desired \\ response \\ $d(n)$}] {} node[pos=0.9] {$+$} (sum);
\draw [draw,->, align=left] (sum) -- node {Estimation \\ error \\ $e(n)$} (error);

\end{tikzpicture}
\caption{Block diagram representation of the statistical filtering problem, adopted from \cite{haykin2002adaptive}.} \label{fig:filtering_problem}
\end{figure}

Assume a stationary stochastic process with known statistical parameters as the mean and correlation functions of the useful signal and the unwanted additive noise. Then, the solution to this statistical optimisation problem is commonly known as the Wiener filter. Yet, since the Wiener filter requires a priori information about the statistics of the data to be processed, it may not be optimum for non-stationary processes.

\section{Adaptive Filters}

A possible approach to mitigate the limitations of the Wiener filter for non-stationary processes is the `estimate and plug' procedure. The filter `estimates' the statistical parameters of the relevant signals and `plugs' them into a non-recursive formula for computing the filter parameters. This procedure requires excessively elaborate and costly hardware for real-time operation \cite{haykin2002adaptive}. To overcome this disadvantage one may use an adaptive filter, which is a self-designing system that relies, in contrast, on a recursive algorithm. It allows the filter to perform satisfactorily even if there is no complete knowledge of the relevant signal characteristics. Provided the variations in the  statistics of the input data are sufficiently slow, the algorithm can track time variations and is thus suitable for non-stationary environments. The algorithm starts from some predetermined set of initial conditions respecting the knowledge about the system. In a stationary environment it converges to the optimum Wiener solution in some statistical sense after successive iterations.

Due to the fact that the parameters of an adaptive filter are updated each iteration, they become data dependent. The system does not obey the principles of superposition which therefore makes the adaptive filter in reality a nonlinear system. However, an adaptive filter is commonly said to be linear if its input-output map satisfies the superposition principle, as long as its parameters are held fixed. Otherwise it is said to be nonlinear.


\section{Kalman Filter}

The Kalman filter is a set of recursive mathematical equations that provides an efficient means to estimate the state of a process, even when the precise nature of the modelled system is unknown. The filter is named after Rudolf E. Kalman who 1960 published his famous paper describing a recursive solution to the discrete-data linear filtering problem \cite{kalman_1960}. Since that time, the Kalman filter has been the subject of extensive research, due, to a large extent, to the advances in digital computing \cite{welch2014}.

\subsection{A Simple Example}

The following simple example from \cite{Maybeck79} is no complete mathematical derivation but an illustrative description of the determination of a one-dimensional position to understand how the Kalman filter works.

Suppose you are lost at sea during the night and take a star sighting to determine your approximate position at time $t_1$ to be $z_1$. Your location estimate is, due to inherent measurement device inaccuracies and human error, somewhat uncertain, and thus assumed to be associated with a standard deviation is $\sigma_{z_1}$. The conditional probability of $x(t_1)$, your actual position at time $t_1$, conditioned on the observed value $z_1$, is depicted in Figure \ref{fig:measurement_z1}. The best estimate of your position, based on this conditional probability density, is

\begin{equation}
  \hat{x}(t_1)=z_1
\end{equation}

\noindent
and the variance of the error in the estimate is

\begin{equation}
  \sigma^2_x(t_1)=\sigma^2_{z_1}\,.
\end{equation}

\begin{figure}
\centering
\begin{tikzpicture}[pile/.style={->, >=stealth'}]

\draw[very thick] plot[samples=200, smooth, domain=-.5:8.5] (\x, {0.2+5/(1.3*sqrt(2*pi))*exp(-((\x-4)^2)/(2*1.3^2))});
  \draw[->] (-1,0) -- (9,0) node[below, pos=0.98] {$x$};
  \draw[->] (0,-0.5) -- (0,3) node[right] {$f_{x(t_1)|z(t_1)}(x|z_1)$};
  \draw[-] (4,0) -- (4,1.74) node[pos=-0.15] {$z_1$};
  \draw[pile, style={->, >=stealth', shorten <=0pt, shorten
    >=0pt}] (4,0.8) -- (2.26,0.8) node[below, pos=0.5] {$\sigma_{z_1}$};
  
\end{tikzpicture}
\caption{Conditional density of position based on measurement value $z_1$, adopted from \cite{Maybeck79}.} \label{fig:measurement_z1}
\end{figure}

Right after you, say a trained navigator friend takes an independent fix at time $t_2 \cong t_1$, so that the true position has not changes at all. He obtains a measurement $z_2$ with a variance $\sigma_{z_2}$, which is somewhat smaller than yours, since he has a higher skill. Figure \ref{fig:measurement_z2} depicts the conditional density of your position at time $t_2$, based only on the measurement value $z_2$. Combining these data, your position at time $t_2 \cong t_1$, $x(t_2)$, given both $z_1$ and $z_2$, is then a Gaussian density with mean $\mu$ and variance $\sigma^2$, as indicated in Figure \ref{fig:combination}, with

\begin{figure}
\centering
\begin{tikzpicture}[pile/.style={->, >=stealth'}]

\draw[thick, dashed] plot[samples=200, smooth, domain=-.5:8.5] (\x, {0.2+5/(1.3*sqrt(2*pi))*exp(-((\x-4)^2)/(2*1.3^2))});
\draw[very thick] plot[samples=200, smooth, domain=3.9:8.1] (\x, {0.1+5/(0.6*sqrt(2*pi))*exp(-((\x-6)^2)/(2*0.6^2))});
  \draw[->] (-1,0) -- (9,0) node[below, pos=0.98] {$x$};
  \draw[->] (0,-0.5) -- (0,4) node[right] {$f_{x(t_2)|z(t_2)}(x|z_2)$};
  \draw[dashed] (4,0) -- (4,1.74) node[pos=-0.15] {$z_1$};
  \draw[pile, style={dashed, ->, >=stealth', shorten <=0pt, shorten
    >=0pt}] (4,0.8) -- (2.26,0.8) node[below, pos=0.5] {$\sigma_{z_1}$};
  \draw[-] (6,0) -- (6,3.43) node[pos=-0.08] {$z_2$};
    \draw[pile, style={->, >=stealth', shorten <=0pt, shorten
    >=0pt}] (6,2) -- (6.62,2) node[below, pos=0.5] {$\sigma_{z_2}$};
  
\end{tikzpicture}
\caption{Conditional density of position based on measurement value $z_2$ alone, adopted from \cite{Maybeck79}.} \label{fig:measurement_z2}
\end{figure}

\begin{figure}
\centering
\begin{tikzpicture}[pile/.style={->, >=stealth'}]


\draw[thick, dashed] plot[samples=200, smooth, domain=-.5:8.5] (\x, {0.2+5/(1.3*sqrt(2*pi))*exp(-((\x-4)^2)/(2*1.3^2))});
\draw[thick, dashed] plot[samples=200, smooth, domain=3.9:8.1] (\x, {0.1+5/(0.6*sqrt(2*pi))*exp(-((\x-6)^2)/(2*0.6^2))});
\draw[very thick] plot[samples=200, smooth, domain=3.8:6.2] (\x, {0.05+5/(0.35*sqrt(2*pi))*exp(-((\x-5)^2)/(2*0.35^2))});
  \draw[->] (-1,0) -- (9,0) node[below, pos=0.98] {$x$};
  \draw[->] (0,-0.5) -- (0,6) node[right] {$f_{x(t_2)|z(t_2)}(x|z_2)$};
  \draw[dashed] (4,0) -- (4,1.74) node[pos=-0.15] {$z_1$};
  \draw[pile, style={dashed, ->, >=stealth', shorten <=0pt, shorten
    >=0pt}] (4,0.8) -- (2.26,0.8) node[below, pos=0.5] {$\sigma_{z_1}$};
  \draw[dashed] (6,0) -- (6,3.43) node[pos=-0.08] {$z_2$};
    \draw[pile, style={dashed, ->, >=stealth', shorten <=0pt, shorten
    >=0pt}] (6,2) -- (6.61,2) node[below, pos=0.5] {$\sigma_{z_2}$};
  \draw[-] (5,0) -- (5,5.75) node[pos=-0.05] {$\mu$};
    \draw[pile, style={->, >=stealth', shorten <=0pt, shorten
    >=0pt}] (5,3.5) -- (4.67,3.5) node[below, pos=0.5] {$\sigma$};
  
\end{tikzpicture}
\caption{Conditional density of position based on data $z_1$ and $z_2$, adopted from \cite{Maybeck79}.} \label{fig:combination}
\end{figure}


\begin{equation}\label{eq:mu}
  \mu=z_1\frac{\sigma^2_{z_2}}{\sigma^2_{z_1}+\sigma^2_{z_2}}+z_2\frac{\sigma^2_{z_1}}{\sigma^2_{z_1}+\sigma^2_{z_2}}
\end{equation}

and

\begin{equation}
  \frac{1}{\sigma^2}=\frac{1}{\sigma^2_{z_1}}+\frac{1}{\sigma^2_{z_2}}\,.
\end{equation}

\noindent
The uncertainty in your estimate of position has been decreased because $\sigma$ is less than either $\sigma^2_{z_1}$ or $\sigma^2_{z_2}$. Even if $\sigma_{z_1}$ was very large, the variance of the estimate is less than $\sigma_{z_2}$, which means that even poor quality data increases the precision of the filter output. The best estimate, given this density, is

\begin{equation}\label{eq:filter_output}
  \hat{x}(t_2)=\mu\,,
\end{equation}

\noindent
with an associated error variance $\sigma^2$.

Having a closer look at the form of $\mu$ in Equation \ref{eq:mu}, one notices that it makes good sense. If the measurements were of equal precision, meaning $\sigma_{z_1}=\sigma_{z_2}$, the optimal estate is simply the average of both measurements, as would be expected. If $\sigma_{z_1}$ is larger than $\sigma_{z_2}$, the equation weights $z_2$ more heavily than $z_1$.

Equation \ref{eq:filter_output} for the filter output can be written as

\begin{equation}\label{eq:x2_hat}
\begin{split}
  \hat{x}(t_2) & =z_1\frac{\sigma^2_{z_2}}{\sigma^2_{z_1}+\sigma^2_{z_2}}+z_2\frac{\sigma^2_{z_1}}{\sigma^2_{z_1}+\sigma^2_{z_2}} \\
  & =z_1+\frac{\sigma^2_{z_1}}{\sigma^2_{z_1}+\sigma^2_{z_2}}[z_2-z_1]
\end{split}
\end{equation}

\noindent
or in a form that is used in Kalman filter implementations, with $\hat{x}(t_1)=z_1$, as

\begin{equation}\label{eq:x2_hat_kalman}
  \hat{x}(t_2) = \hat{x}(t_1) + K(t_2)[z_2-\hat{x}(t_1)]\,,
\end{equation}

\noindent
where

\begin{equation}\label{}
  K(t_2) = \frac{\sigma^2_{z_1}}{\sigma^2_{z_1}+\sigma^2_{z_2}}\,.
\end{equation}

\noindent
These equation represent the `predictor-corrector' structure of the Kalman filter. A prediction of the value that the desired variables and the measurements will have at the next measurement time is made, based on all previous information. Then the difference between the measurement and its predicted value is used to correct the prediction of the desired variables. According to Equation \ref{eq:x2_hat_kalman} the optimal estimate at time $t_2$, $\hat{x}(t_2)$, is equal to $\hat{x}(t_1)$, the best prediction of its value before $z_2$ is taken, plus a correction term of an optimal weighting value times the difference between $z_2$ and the best prediction of it before the measurement is actually taken.

To incorporate dynamics into the model, suppose you travel for some time before taking another measurement. The best model you have for your motion may be of the form

\begin{equation}\label{}
  \frac{dx}{dt} = u + w\,,
\end{equation}

\noindent
where $u$ is a nominal velocity and $w$ is a noise term, representing the uncertainty in your knowledge of the actual velocity due to disturbances and effects not accounted for in the simple first order equation. It will be modeled as white Gaussian noise with a mean of zero and variance of $\sigma^2_w$.

\begin{figure}
\centering
\begin{tikzpicture}[pile/.style={->, >=stealth'}]

\draw[very thick] plot[samples=200, smooth, domain=0.8:3.2] (\x, {0.05+5/(0.35*sqrt(2*pi))*exp(-((\x-2)^2)/(2*0.35^2))});
\draw[very thick] plot[samples=200, smooth, domain=2.2:7.8] (\x, {0.2+5/(0.7*sqrt(2*pi))*exp(-((\x-5)^2)/(2*0.7^2))});
\draw[very thick] plot[samples=200, smooth, domain=4.5:11.5] (\x, {0.4+5/(1.3*sqrt(2*pi))*exp(-((\x-8)^2)/(2*1.3^2))});

  \draw[->] (-0.5,0) -- (12,0) node[below, pos=0.98] {$x$};
  \draw[->] (0,-0.5) -- (0,6.5) node[right] {$f_{x(t)|z(t_1), z(t_2)}(x|z_1, z_2)$};
  \draw[-] (8,0) -- (8,1.94) node[pos=-0.15] {$\hat{x}(t^-_3)$};
  \draw[pile, style={->, >=stealth', shorten <=0pt, shorten
    >=0pt}] (8,0.8) -- (10.07,0.8) node[below, pos=0.5] {$\sigma_x(t^-_3)$};
  \draw[-] (5,0) -- (5,3.07) node[pos=-0.09] {$\hat{x}(t)$};
    \draw[pile, style={->, >=stealth', shorten <=0pt, shorten
    >=0pt}] (5,2) -- (5.65,2) node[right, pos=0.98] {$\sigma_x(t)$};
  \draw[-] (2,0) -- (2,5.75) node[pos=-0.05] {$\hat{x}(t_2)$};
    \draw[pile, style={->, >=stealth', shorten <=0pt, shorten
    >=0pt}] (2,3.5) -- (2.33,3.5) node[right, pos=0.95] {$\sigma_x(t_2)$};
  
\end{tikzpicture}
\caption{Propagation of conditional probability density, adopted from \cite{Maybeck79}.} \label{fig:propagation}
\end{figure}

The conditional density of the position at time $t_2$, given $z_1$ and $z_2$, was previously derived. Figure \ref{fig:propagation} shows graphically how the density travels along the x-axis as time progresses. It start at the best estimate and moves according to the above mentioned model of dynamics. Due to the constant addition of uncertainty over time it spreads out. As the variance becomes greater you become less sure of your position. The Gaussian density $f_{x(t_3)|z(t_1), z(t_2)}(x|z_1, z_2)$ can be expressed mathematically by its mean and variance given by

\begin{equation}\label{eq:system_model}
  \hat{x}(t^-_3)=\hat{x}(t_2)+u[t_3-t_2]
\end{equation}

\begin{equation}\label{eq:variance_predicted}
  \sigma^2_x(t^-_3)=\sigma^2_x(t_2)+\sigma^2_w[t_3-t_2]
\end{equation}

\noindent
$\hat{x}(t^-_3)$ is the optimal prediction of the location at $t^-_3$ before the measurement is taken at $t_3$, associated with the variance in this prediction, $\sigma^2_x(t^-_3)$.

Now a measurement $z_3$ with an assumed variance $\sigma^2_{z_3}$ is taken. As before, its conditional probability density is combined with the density with mean $\hat{x}(t^-_3)$ and variance $\sigma^2_x(t^-_3)$, to yield a Gaussian density with mean

\begin{equation}\label{eq:estimation_kalman}
  \hat{x}(t_3) = \hat{x}(t^-_3) + K(t_3)[z_3-\hat{x}(t^-_3)]
\end{equation}

\noindent
and variance

\begin{equation}\label{eq:variance_kalman}
  \sigma^2_x(t_3) = \sigma^2_x(t^-_3)-K(t_3)\sigma^2_x(t^-_3)\,,
\end{equation}

\noindent
where the gain $K(t_3)$ is given by

\begin{equation}\label{eq:gain_kalman}
  K(t_3) = \frac{\sigma^2_x(t^-_3)}{\sigma^2_x(t^-_3)+\sigma^2_{z_3}}\,.
\end{equation}

Observing the form of Equation \ref{eq:gain_kalman} the reasonableness of the filter structure becomes obvious. If the variance of the measurement noise $\sigma^2_{z_3}$ is large, then $K(t_3)$ is small, meaning that little confidence is put in a very noisy measurement and that it is weighted lightly. For $\sigma^2_{z_3}\rightarrow\infty$, $K(t_3)$ becomes zero, and $\hat{x}(t_3)$ equals $\hat{x}(t^-_3)$. Thus, an infinitely noisy measurement is totally ignored. Likewise, if the dynamical system noise variance $\sigma^2_w$ is large, then according to Equation \ref{eq:variance_predicted}, $\sigma^2_x(t^-_3)$ will be large, and so will be $K(t_3)$. Therefore, the measurement is weighted heavily, in case you are not very certain about the output of the system model within the filter structure. In the limit as $\sigma^2_w \rightarrow\infty$, $\sigma^2_x(t^-_3) \rightarrow\infty$, and $K(t_3) \rightarrow1$, so Equation \ref{eq:filter_output} yields

\begin{equation}\label{eq:prediction_kalman}
  \hat{x}(t_3) = \hat{x}(t^-_3) + 1 \cdot [z_3-\hat{x}(t^-_3)] = z_3\,.
\end{equation}

\noindent
That means that in the limit of absolutely no confidence in the system model output, solely the new measurement is taken as the optimal estimate. Finally, if you are absolutely sure of your estimate before $z_3$ comes available, $\sigma^2_x(t^-_3)$ would become zero, and so would $K(t_3)$, which means that the measurements would be left disregarded. 

Extending Equation \ref{eq:system_model} and \ref{eq:variance_predicted} to the vector case and allowing time varying parameters in the system and noise description leads to the general Kalman filter algorithm depicted in Figure \ref{}. The update phase at the measurement time corresponds to the extension of Equation \ref{eq:estimation_kalman} and \ref{eq:gain_kalman}.

\subsection{The Kalman Equations}


\section{Extended Kalman Filter}


